{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <a id=\"top_section\"></a>\n\n<div align='center'><font size=\"6\" color=\"#000000\"><b>NLP with disaster tweets!(BERT explained) <br>(~84.5% Accuracy)</b></font></div>\n<hr>\n<div align='center'><font size=\"5\" color=\"#000000\">About the problem</font></div>\n<hr>\n\nIn this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t.<br>\nI have two notebooks on this competition , the first one is using basic naive-base model whereas this one is by using BERT pre-trained model. If you're a beginner I highly recommend you to check out the basic model notebook first ! Here is the link <br>\n#### [NLP w/ Disaster tweets!(Explained)](https://www.kaggle.com/friskycodeur/nlp-w-disaster-tweets-explained)\n<br>\n<img src='https://c7.uihere.com/files/932/486/348/tweet-bird-logotwitter-icon-buttonflat-social-vector.jpg' height=500 width=500>\n<br>\n\n### Here are the things I will try to cover in this Notebook:\n\n- A brief introduction to BERT\n- Defining helper functions\n- Data cleaning\n- Using BERT model to get higher accuracy\n- Is it posible to get 100% accuracy and if yes then how can you get that !\n\n### If you liked this kernel feel free to upvote and leave feedback, thanks!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\" align='center'>Table of Content</h3>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#top_section\" role=\"tab\" aria-controls=\"profile\">About the Problem<span class=\"badge badge-primary badge-pill\">1</span></a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#bert\" role=\"tab\" aria-controls=\"messages\">A brief intoduction to BERT<span class=\"badge badge-primary badge-pill\">2</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#sec1\" role=\"tab\" aria-controls=\"messages\">Importing basic libraries and data<span class=\"badge badge-primary badge-pill\">3</span></a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#sec2\" role=\"tab\" aria-controls=\"settings\">Defining helpful functions<span class=\"badge badge-primary badge-pill\">4</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#sec3\" role=\"tab\" aria-controls=\"settings\">Data Cleaning<span class=\"badge badge-primary badge-pill\">5</span></a> \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#sec4\" role=\"tab\" aria-controls=\"settings\">Pre-training BERT<span class=\"badge badge-primary badge-pill\">6</span></a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#sec5\" role=\"tab\" aria-controls=\"settings\">Modelling<span class=\"badge badge-primary badge-pill\">7</span></a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#sec6\" role=\"tab\" aria-controls=\"settings\">Submission<span class=\"badge badge-primary badge-pill\">8</span></a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#sec7\" role=\"tab\" aria-controls=\"settings\">The secret to 100% accuracy<span class=\"badge badge-primary badge-pill\">9</span></a>    \n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#sec8\" role=\"tab\" aria-controls=\"settings\">References and Some last words<span class=\"badge badge-primary badge-pill\">10</span></a>  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let us start with some basic understandings.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> If you want to read about the EDA of this data and want to see how it can be implemented using Naive-bayes approach then refer this notebook >>> [NLP w/ Disaster tweets!(Explained)](https://www.kaggle.com/friskycodeur/nlp-w-disaster-tweets-explained)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"bert\"></a>\n<h1 align='left'> BERT </h1>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"BERT stands for Bidirectional Encoder Representation from Transformers. In a transformer flow if we stack a number of encoders then we get a BERT. It is easier to make BERT understand a language. BERT also has a variety of problems such as Question-Answering , Sentiment Analysis , Text sumamrzation ,etc. <br>\nSteps to use a BERT model : <br>\n- Pretraining BERT : To understand language\n- Fine tune BERT : To help us in our specific task\n\n#### Pretraining BERT \n- To make BERT learn what is language.\n- It has two part Masked Langauge Modelling(MLM) and Next Sentence Prediction(NSP).\n- Both of these problems are trained simultaneously.\n\n#### Fine tuning BERT\n- It is a quiet fast process.\n- Only the output parameters are leant from scratch and whereas the rest of the parameters are slightly fine-tuned and not that   much changed which in turn makes the process faster.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If you want to read more about BERT. [Click Here.](https://arxiv.org/pdf/1810.04805.pdf)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec1\"></a>\n## Importing required libraries and Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use the official tokenization script created by the Google team\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will start by importing the libraries to be used and the dataset provided.","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport re\nimport tokenization\nimport string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec2\"></a>\n## Defining helpful functions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this part we will define some functions , which we can use later to make the process smoother. <br><br>\nFirst we will create a function which takens input the text we want to work on , the tokenizer we are using . This will help us to encode the text for BERT using tokenizer and will give us tokens, masks and segments !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will build a mdel-building function , in which we  will input the layer/bert-layer and get the model as an output.","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec3\"></a>\n## Data cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We are here at the data cleaning part.<br><br>\nFirst off let's convert everything in lowercase.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def lowercase_text(text):\n    return text.lower()\n\ntrain.text=train.text.apply(lambda x: lowercase_text(x))\ntest.text=test.text.apply(lambda x: lowercase_text(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will remove the text noises.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_noise(text):\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ntrain.text=train.text.apply(lambda x: remove_noise(x))\ntest.text=test.text.apply(lambda x: remove_noise(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how our data looks now. Must be cleaner.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.text.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec4\"></a>\n## Pre-training BERT\n\n- First we will load bert from tensorhub\n- From the bert-layer we will load the tokenizer\n- We will encode and convert the data into Bert-input form","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will load the tokenizer from our bert layer now !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's encode our text using this tokenizer and the **bert_encoder** function we created earlier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec5\"></a>\n## Modelling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We have done the pre-training of the model , now we will build our model using BERT.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's train our model now and see how it's doing.<br>\nYou can try and play with epochs and batch_size to see if you can get better accuracy.<br>\nThat's how i got to 84.5 from 82 ;)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=2,\n    callbacks=[checkpoint],\n    batch_size=15\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics=pd.DataFrame(model.history.history)\nmetrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec6\"></a>\n## Submission","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We have succesfully pre-train,buid and train our model.<br>\nAnd now is the time to get the predictions and submit our solution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec7\"></a>\n## The secret to 100% accuracy\n\n100% accuracy is not really that possible in this problem, as far as i have seen with my models and by reading the kernels of other kagglers , but still you can see there are people on the top with 100% accuracy, so what is the secret of 100% accuracy?\n<br>\nThe secret is simply a leaked labels. The labels of the test set which is being used for estiamtion of our score is available on some other site and hence alot of people are using that to get that 100% accuracy mark.\n<br>\nThis competition is for leaning purpose only , not for ranks or anything.<br>\nHere is a notebook explaining it [A Real Disaster - Leaked Label](https://www.kaggle.com/szelee/a-real-disaster-leaked-label)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"sec8\"></a>\n## References\n<br>\n\n- [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert)\n- [Disaster NLP: Keras BERT using TFHub](https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Some last words:\n\nThank you for reading! I'm still a beginner and want to improve myself in every way I can. So if you have any ideas to feedback please let me know in the comments section!\n\n\n<div align='center'><font size=\"3\" color=\"#000000\"><b>And again please star if you liked this notebook so it can reach more people, Thanks!</b></font></div>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src='https://thumbs.gfycat.com/EnormousRegularClam-size_restricted.gif'>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}